---
title: "NOAA Nearshore Fish Atlas, Exploratory Data Analyses"
author: "Chris Guo"
date: "Last compiled on `r format(Sys.time(), '%d %B, %Y')`"
output:
  html_document:
    toc: TRUE
    toc_depth: 2
    toc_float:
      collapsed: FALSE
      print: FALSE
    number_sections: TRUE
    code_download: TRUE
theme: "flatly"
---

```{r include = FALSE}
knitr::opts_chunk$set(message = FALSE)
knitr::opts_chunk$set(warning = FALSE)
knitr::opts_chunk$set(size = "scriptsize")
```

Built with R version `r getRversion()`.

# Introduction

The purpose of this document is to make open and share-able the research methods used for my dissertation on nearshore fish communities in Alaska working towards a PhD in marine biology at the University of Alaska Fairbanks. Here, I cover steps of the data preparation for my second and third chapters concerning spatial and temporal distributions of nearshore fishes across the state. This and other files can be accessed via the Kachemak Bay National Estuarine Research Reserve's github [nearshore fish repository](https://github.com/kbnerr/nearshore-fish).

In this document I share exploratory data analyses conducted on the NOAA Nearshore Fish Atlas (NFA) database. The NFA database can be found here, <https://alaskafisheries.noaa.gov/mapping/sz/>. Namely, this document contains the steps taken after cleaning/wrangling raw data. In particular I produce various visualizations of the data in space and time. Initial wrangle steps can be viewed at this [Rpubs page](https://rpubs.com/chguo1/1188614) and its data objects are sourced below (file "NFA.rda").

## Set up

Load required packages, define directory, set options, source/load files:

```{r results = 'hide'}
# Packages
library(tidyverse)
library(lubridate)
library(here)
library(leaflet)
library(RColorBrewer)

# Directory
wd = here()
dirs = wd %>% list.files() %>% str_subset(pattern = "^README|^LICENSE|.md$|.Rproj$", negate = TRUE)
for (i in seq_along(dirs)) {
  name = str_replace_all(dirs[i], "^", "dir.")
  path = str_replace_all(dirs[i], "^", str_c(wd, "/"))
  assign(name, path)
  rm(name, path, i)
}

# Options

# Source/Load
load(file.path(dir.data, "NFA_wrangled.rda")) # wrangled data
```

## Background and objectives

Based on my own research and related literature, I can broadly interpret these beach seine data in a spatiotemporal context. Depending on when (time of year) and where (location and habitat), I have a general structure in mind of the diversity of the community. Depending on who's there, I have a rough guess of the relative abundance of each community member (at least for the more commonly caught ones). My hope with working with the NFA data is that this broad understanding can be formally tested with inference and statistics, and that more informative research questions can be answered.

Findings from other nearshore fish researchers largely agree that seasonality (or some related environmental condition) is a strong predictor of community or species presence. Interannual differences can exhibit wide variability and should be accounted for whenever possible. And at larger time-scales (e.g., decadal), communities exhibit changes which may relate to regime shifts occurring more broadly than the nearshore.

Spatially, large-scale (regional) effects appear to be as or more important than sub-regional or local-scale effects. However, many studies also find significant relationships in community response at these smaller scales. Likely, the most appropriate spatial consideration depends on the question being asked. So maybe a more appropriate question to ask of the NFA data is if there any significant spatial scales discernible in the data, and also how should we address those scales in future research, such as studies on subsets of the taxa or habitat management considerations.

I'll start by exploring the structure of our 'visits' dataframe, and then I'll move on to visualizing the information derived from out 'catch' dataframe.

# Exploring visit data

## Summary of visits

Remember that in the data wrangling stage, we created a new sample identifier based on site and date which we called VisitID. Let's take a look at the variables associated with each VisitID:

```{r}
glimpse(visits)
```

During the wrangling stage, I decided that Replicates and MeshSize should be the main covariates that needed consideration. Cluster, on the other hand, is a byproduct of clustering some sites together, so it may not be needed if we're using Lat/Lon as our primary explanatory variable. Still, it could be a useful grouping factor later on. Date will likely be mutated into multiple additional periods (day of year, month, year) to see if any of those factors are useful in explaining fish data. We can create those variables as they come up in our exploration.

## Spatial distribution

```{r}
leaflet(visits) %>% 
  addTiles(options = tileOptions(minZoom = 3.5,
                                 zIndex = 0.5)) %>%
  setView(lat = 65, lng = -152, zoom = 3.5) %>%
  addSimpleGraticule(interval = 5) %>%  
  addCircleMarkers(lng = ~Lon, lat = ~Lat,
                   stroke = FALSE,
                   popup = ~VisitID,
                   clusterOptions = markerClusterOptions())
```

In the map above we can view all of our samples from across Alaska. Leaflet options are nice because you can optionally group samples on the map and show a boundary polygon of the area they cover when you hover over their icon. At the minimum zoom level (3.5), we see the largest group spanning nearly all of Southeast Alaska, followed by a group in Southcentral that covers Prince William Sound, Cook Inlet, and the northern half of Kodiak Island. The third largest group forms a much smaller polygon area around Utqiagvik. From there, we see even smaller groups from the Eastern Beaufort Sea around Kaktovik, the Bering Strait and Southern Chukchi Sea near Kotzebue, and Bristol Bay. There are also two groups from the Alaska Peninsula and Aleutian Islands, one focused around Adak and another spanning from Sand Point to Unalaska. We can see how subsets of the samples change by zooming in. When there are single samples in view, hovering over those will tell us its VisitID name.

Obviously, this visualization of the data is not perfect and pretty rough- note that the groups are formed by the overlap of samples based on pixel radius (10). For example, the polygon of samples from Bristol Bay at the minimum zoom level actually crosses the Alaska Peninsula to include a sample from Aghiyuk Island. Although, the map is useful in gaining a general understanding of the existing clusters of samples within the NFA database.

## Regions

We may want to define a spatial factor based on these rough groupings as a starting place to see if the smaller, more isolated groups should be lumped with other groups or not. Actually, the NFA already has a Region classifier per site, but the reason I do not want to use it is because they linked Region to SiteID. When we created our new VisitID, some samples now contained two Region labels- these were mostly cases from around Utqiagvik where some were labelled as Chuckchi and others as Beaufort but actually occurred on the same day and within a very short distance of each other.

Instead of using the the given regional classes, let's just make our own variable called 'Region' and add it to the our visits df:

```{r}
# Start with events so that we can make use of Location info for unique cases,
regions = events %>%
  select(EventID, VisitID, Lat, Lon, Location) %>%
  mutate(Region = case_when(Lat > 69 & Lat < 80 & Lon > -150 & Lon < -140 ~ "Beaufort East",
                            Lat > 69 & Lat < 80 & Lon > -160 & Lon < -150 ~ "Chukchi/Beaufort",
                            Lat > 65 & Lat < 69 & Lon > -170 & Lon < -160 ~ "Chukchi South",
                            Lat > 57 & Lat < 60 & Lon > -165 & Lon < -155 ~ "Bristol Bay",
                            Lat > 50 & Lat < 55 & Lon > -180 & Lon < -175 ~ "Aleutians Adak",
                            Lat > 50 & Lat < 55 & Lon > -170 & Lon < -165 ~ "Aleutians Unalaska",
                            Lat > 55 & Lat < 65 & Lon > -155 & Lon < -145 ~ "GOA Southcentral",
                            Lat > 53 & Lat < 61 & Lon > -148 & Lon < -130 ~ "GOA Southeast"))

# Check which samples we missed,
filter(regions, is.na(Region))

# Let's address those using Location,
regions = mutate(regions,
       Region = case_when(!is.na(Region) ~ Region,
                          is.na(Region) & str_detect(Location, "Yunaska") ~ "Aleutians Yunaska",
                          is.na(Region) & str_detect(Location, "Aghiyuk") ~ "Aleutians Aghiyuk",
                          is.na(Region) & str_detect(Location, "Shumagin") ~ "Aleutians Shumagin"))

# Check again (should be 0),
filter(regions, is.na(Region)) %>% nrow()
```

Now let's add Region as a factor to our visits df, and show it on a map. Since we're manipulating our data objects again, let's also rename visits so that we can keep track of our changes. 

```{r}
# Call our wrangled df version visits.0 and remove the non-version 'visits' if it's there,
if ("visits" %in% ls()) assign("visits.0", visits)
if ("visits" %in% ls()) rm(visits)

# Make Region a factor and join by VisitID,
visits.1 = regions %>%
  select(VisitID, Region) %>%
  distinct() %>%
  mutate(Region = as.factor(Region)) %>%
  left_join(visits.0, ., by = "VisitID")

# Function for regional palette
palette.region = colorFactor(palette = brewer.pal(n = visits.1$Region %>% n_distinct(),
                                                  name = 'Spectral'),
                             domain = factor(visits.1$Region))

# Map
leaflet(visits.1) %>% 
  addTiles() %>%
  addCircleMarkers(lng = ~Lon, lat = ~Lat,
                   color = ~palette.region(Region),
                   fillColor = ~palette.region(Region),
                   popup = ~paste(Region, VisitID),
                   stroke = FALSE) %>%
  addLegend(position = "topright",
            pal = palette.region, values = ~Region,
            title = "Region",
            opacity = 0.75)
```

```{r echo = FALSE}
# Number of samples per Region
select(visits.1, VisitID, Region) %>%
  count(Region) %>%
  distinct() %>%
  ggplot(., aes(x = Region, y = n, fill = Region)) +
  geom_col() +
  geom_text(aes(label = n), vjust = -0.5) +
  scale_fill_brewer(palette = "Spectral") +
  labs(x = NULL, y = NULL, title = "No. samples per Region") +
  theme(axis.text.x = element_blank(),
        axis.ticks.x = element_blank())
```

Like we saw before, the majority of our samples are located in the Gulf of Alaska (GOA), namely Southeast (SEAk) and Southcentral (SCAK). If we were to look at differences in catach in the GOA, it'd be cool if we could include the Aleutian samples too. Obviously, we'll want to address those few instances from Aghiyuk, Shumagin, and Yunaska, and ideally we aggregate them into one or two groups. We also see relatively small sample sizes from Bristol Bay and from East Beaufort, which may or may not be prohibitive in conducting meaningful tests. One other consideration is the couple of samples along the Bering Strait coast which are lumped together with a larger concentration of samples from South Chukchi.

Some of our first analyses should see if the more isolated samples are different from the other samples. If not, then we can aggregate. If yes, then we should report any significant tests.

## Temporal distribution

Next we'll want to take a look at how our samples distribute throughout various time periods. This will give us a good idea of how well our samples may or may not overlap- particularly, we'll want to see overlaps in years and months for better comparisons of the catch data.

```{r}
# First create a df specifically containing at time variables
times = visits.1 %>%
  select(VisitID, Date) %>%
  mutate(Year = year(Date),
         Month = month(Date, label = TRUE),
         Week = week(Date),
         Day = yday(Date))

# Year
times %>%
  count(Year) %>%
  ggplot(., aes(x = Year, y = n)) +
  geom_col() +
  geom_text(aes(label = n), vjust = -0.5, color = "blue") +
  labs(x = NULL, y = NULL, title = "No. samples per Year")

# Year without 1976
times %>%
  filter(Year > 1980) %>%
  count(Year) %>%
  ggplot(., aes(x = Year, y = n)) +
  geom_col() +
  geom_text(aes(label = n), vjust = -0.5, color = "blue") +
  labs(x = NULL, y = NULL, title = "No. samples per Year without 1976")

# Month
times %>%
  count(Month) %>%
  ggplot(., aes(x = Month, y = n)) +
  geom_col() +
  geom_text(aes(label = n), vjust = -0.5, color = "blue") +
  labs(x = NULL, y = NULL, title = "No. samples per Month")

# Week
times %>%
  count(Week) %>%
  ggplot(., aes(x = Week, y = n)) +
  geom_col() +
  geom_text(aes(label = n), hjust = -0.15, angle = 90, color = "blue") +
  labs(x = NULL, y = NULL, title = "No. samples per Week")

# Day of Year
times %>%
  count(Day) %>%
  distinct() %>%
  ggplot(., aes(x = Day, y = n)) +
  geom_col() +
  labs(x = NULL, y = NULL, title = "No. samples per Day of Year")
```

By year, we see a large variability in number of samples, as well as a lone 1976 year which I recognize to be Blackburn's dataset from Lower Cook Inlet. That 1976 is an interesting dataset that could be useful for a look at catches over time in that specific area, but it'll need to be excluded from any other comparisons. The lowest number of samples occur in 2010, 2020, and 2021. We may want to pay close attention to these years in case they show up as outliers. It seems that 1996-2000 is a period of heavy sampling, followed by the mid 2010s and somewhat active year in 2006.

By month, we see an obvious uptick during the summer which makes sense given winter conditions along most of Alaska's coast (light, ice, etc.). March and October are interesting months because of the potential for impacts on fishes during years with warmer shoulder seasons. November thru February will likely be removed for our purposes. Our key months look to be June, July and August, with possible inclusion of April, May, and September.

It's interesting if we look at the by-week and by-day graphs, we see a dip in sampling around mid-June/early-July. Not sure why this is- maybe related to more targeting of juvenile salmonids in early June, or maybe so reduced effort around the 4th of July holiday.

## Monthly and yearly sampling by region

Let's now see how our samples overlap by Region and Year, and by Region and Month. For these purposes, let's combine the Aleutians into one group for now, and remove the 1976 data.

```{r}
# Samples by Year and Region,
left_join(filter(times, Year > 1980),
          select(visits.1, VisitID, Region), by = "VisitID") %>%
  mutate(Region = case_when(str_detect(Region, "Aleutians") ~ "Aleutians",
                            !str_detect(Region, "Aleutians") ~ Region)) %>%
  ggplot(aes(x = Date, y = Region, color = Region)) +
  geom_boxplot() +
  geom_jitter(size = 0.5, alpha = 0.75) +
  stat_summary(geom = "text", vjust = -1.9, color = "black",
               fun.data = function(x) {
                 return(c(y = median(x), label = length(x)))
               }) +
  scale_color_brewer(palette = "Spectral") +
  scale_x_date(date_labels = "%y",
               date_breaks = "2 years") +
  labs(x = "Year", y = "Region",
       title = "Samples by Year and Region",
       subtitle = "No. samples labelled over median") +
  theme_minimal() +
  theme(legend.position = "none")

# Samples by Month and Region,
left_join(filter(times, Year > 1980),
          select(visits.1, VisitID, Region), by = "VisitID") %>%
  mutate(Region = case_when(str_detect(Region, "Aleutians") ~ "Aleutians",
                            !str_detect(Region, "Aleutians") ~ Region)) %>%
  ggplot(aes(x = Month, y = Region, color = Region)) +
  geom_boxplot() +
  geom_jitter(size = 0.5, alpha = 0.75) +
  stat_summary(geom = "text", vjust = -1.9, color = "black",
               fun.data = function(x) {
                 return(c(y = median(x), label = length(x)))
               })+
  scale_color_brewer(palette = "Spectral") +
  labs(x = "Month", y = "Region",
       title = "Samples by Month and Region",
       subtitle = "Excluding 1976, No. samples labelled by month") +
  theme_minimal() +
  theme(legend.position = "none")
```



```{r}
palette.mesh = colorFactor(palette = brewer.pal(n = visits.1$MeshSize %>% n_distinct(),
                                                name = 'Spectral'),
                           domain = factor(visits.1$MeshSize))

leaflet(visits.1) %>% 
  addTiles() %>%
  addCircleMarkers(lng = ~Lon, lat = ~Lat,
                   radius = ~(sqrt(Replicates*15)),
                   color = ~palette.mesh(MeshSize),
                   fillColor = ~palette.mesh(MeshSize),
                   popup = ~VisitID,
                   stroke = FALSE) %>%
  addLegend(position = "bottomright",
            pal = palette.mesh, values = ~MeshSize,
            title = "Mesh Size (mm)",
            opacity = 0.75)
```

```{r}
palette.mesh.bin = colorBin(palette = brewer.pal(n = 4, name = 'Spectral'),
                            domain = visits.1$MeshSize %>% unique(),
                            bins =  c(0, 4, 7, 11, 13))

leaflet(visits.1) %>% 
  addTiles() %>%
  addCircleMarkers(lng = ~Lon, lat = ~Lat,
                   color = ~palette.mesh.bin(MeshSize),
                   fillColor = ~palette.mesh.bin(MeshSize),
                   popup = ~VisitID,
                   stroke = FALSE) %>%
  addLegend(position = "bottomright",
            colors = c(palette.mesh.bin(3),
                       palette.mesh.bin(6),
                       palette.mesh.bin(9),
                       palette.mesh.bin(12)),
            values = ~MeshSize,
            title = "Mesh Size (mm)",
            opacity = 0.75,
            labels = c("3 or 3.2", "6 or 6.4", "9.5 or 10", "12.7"))
```

# Kachemak Bay

```{r}
KB_events = str_which(events$Location, "Kachemak")
KB.events = events[KB_events, ]
KB.76 = filter(KB.events, year(Date) < 1980)
```
